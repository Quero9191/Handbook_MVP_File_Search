"""
Smart Sync: sincroniza cambios incrementales del KB con Gemini File Search Store.

ğŸ”‘ ARQUITECTURA CON sync_state.json:

El problema: Google File Search Store API no persiste display_name ni permite
identificar quÃ© documento viejo corresponde a quÃ© hash nuevo.

La soluciÃ³n: Mantener un mapeo local en sync_state.json:

{
  "kb/path/to/file.md": {
    "hash": "6a64ced5e0a2c867...",
    "store_doc_id": "fileSearchStores/.../documents/xyz123"
  }
}

Flujo:
1. Cargar sync_state.json (estado anterior)
2. Calcular hash de cada .md en kb/
3. Para cada archivo:
   - Sin cambios â†’ saltar
   - Hash nuevo â†’ BORRAR viejo (por store_doc_id) y SUBIR nuevo
   - Nuevo archivo â†’ SUBIR
4. Detectar eliminados (en sync_state pero no en kb/)
5. Guardar sync_state.json con nuevo estado

GarantÃ­as:
âœ… Nunca duplica
âœ… Detecta cambios
âœ… No depende de API instable
âœ… IdentificaciÃ³n 100% certera (path + hash + Store ID)
"""

import os
import hashlib
import json
import logging
from pathlib import Path
from typing import Dict, Tuple, List

import yaml
from dotenv import load_dotenv
from google import genai

# =========
# Config & Logging
# =========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

ROOT = Path(__file__).resolve().parent
ENV_PATH = ROOT / ".env"
KB_DIR = ROOT / "kb"
STATE_FILE = ROOT / "sync_state.json"  # â† Archivo persistente en Git
STATE_BASE_FILE = ROOT / "sync_state_base.json"  # â† Template base (vacÃ­o)

# Cargar env variables
if not os.getenv("GEMINI_API_KEY"):
    load_dotenv(ENV_PATH)

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
STORE_NAME = os.getenv("FILE_SEARCH_STORE_NAME", "").strip()
STORE_DISPLAY_NAME = os.getenv("STORE_DISPLAY_NAME", "zigchain-handbook-mvp").strip()

if not GEMINI_API_KEY:
    raise RuntimeError("âŒ Falta GEMINI_API_KEY en .env o en GitHub Actions secrets")
if not KB_DIR.exists():
    raise RuntimeError(f"âŒ No existe la carpeta kb/: {KB_DIR}")

logger.info(f"ğŸ“Œ Config:")
logger.info(f"   STORE_NAME: {STORE_NAME[:50]}..." if STORE_NAME else "   STORE_NAME: (crear nuevo)")
logger.info(f"   STORE_DISPLAY_NAME: {STORE_DISPLAY_NAME}")
logger.info(f"   KB_DIR: {KB_DIR}")

client = genai.Client(api_key=GEMINI_API_KEY)

# =========
# Helpers
# =========

def sha256_text(s: str) -> str:
    """Calcula hash SHA256 de un texto"""
    return hashlib.sha256(s.encode("utf-8", errors="ignore")).hexdigest()


def parse_frontmatter(md_text: str) -> Tuple[Dict, str]:
    """Extrae YAML frontmatter entre --- ... ---"""
    text = md_text.lstrip()
    if not text.startswith("---"):
        return {}, md_text

    lines = md_text.splitlines()
    if not lines or lines[0].strip() != "---":
        return {}, md_text

    end = None
    for i in range(1, min(len(lines), 300)):
        if lines[i].strip() == "---":
            end = i
            break
    if end is None:
        return {}, md_text

    fm_raw = "\n".join(lines[1:end])
    body = "\n".join(lines[end + 1:])
    try:
        data = yaml.safe_load(fm_raw) or {}
        if not isinstance(data, dict):
            data = {}
        return data, body
    except Exception as e:
        logger.warning(f"âš ï¸ Error parsing frontmatter: {e}")
        return {}, md_text


def delete_document(store_doc_id: str) -> bool:
    """Borra un documento del File Search Store por su ID (con force=true si es necesario)"""
    if not store_doc_id:
        logger.warning(f"   âš ï¸ Sin ID para borrar (ignorando)")
        return False
    
    try:
        logger.info(f"   ğŸ—‘ï¸  Borrando documento: {store_doc_id[:60]}...")
        client.file_search_stores.delete(
            name=store_doc_id
        )
        logger.info(f"   âœ… Documento borrado")
        return True
    except Exception as e:
        # Si falla por "non-empty", intentar con force=true
        if "non-empty" in str(e).lower() or "FAILED_PRECONDITION" in str(e):
            logger.info(f"   âš ï¸ Documento tiene chunks, borrando con force=true...")
            try:
                import requests
                api_key = os.getenv("GEMINI_API_KEY")
                url = f"https://generativelanguage.googleapis.com/v1beta/{store_doc_id}"
                params = {"key": api_key, "force": "true"}
                resp = requests.delete(url, params=params, timeout=30)
                if resp.status_code == 200:
                    logger.info(f"   âœ… Documento borrado (force=true)")
                    return True
                else:
                    logger.warning(f"   âš ï¸ Error con force=true: {resp.status_code}")
                    return False
            except Exception as force_err:
                logger.warning(f"   âš ï¸ No se pudo borrar ni con force: {force_err}")
                return False
        else:
            logger.warning(f"   âš ï¸ No se pudo borrar: {e}")
            return False


# =========
# State Management
# =========

def load_sync_state() -> Dict[str, dict]:
    """
    Carga el estado anterior: {kb_path -> {"hash": str, "store_doc_id": str}}
    
    Compatible con versiÃ³n antigua que solo tenÃ­a hashes (strings).
    
    En GitHub Actions (primera ejecuciÃ³n):
    - Si sync_state.json estÃ¡ vacÃ­o o no existe
    - Usa sync_state_base.json como base (tambiÃ©n vacÃ­o)
    """
    if STATE_FILE.exists():
        try:
            data = json.loads(STATE_FILE.read_text())
            
            # Si el archivo estÃ¡ vacÃ­o o es un dict vacÃ­o
            if not data:
                logger.info(f"ğŸ“ Primer run detectado - usando template base")
                if STATE_BASE_FILE.exists():
                    data = json.loads(STATE_BASE_FILE.read_text())
            
            # Convertir formato antiguo (solo strings) al nuevo (dicts)
            new_format = {}
            for path, value in data.items():
                if isinstance(value, str):
                    # Formato antiguo: solo el hash
                    new_format[path] = {
                        "hash": value,
                        "store_doc_id": None,  # No lo tenemos del formato anterior
                    }
                else:
                    # Formato nuevo: ya es un dict
                    new_format[path] = value
            
            return new_format
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error loading sync_state.json: {e}")
            return {}
    logger.info(f"ğŸ“ Primer run: sin estado anterior (archivo no existe)")
    return {}


def save_sync_state(state: Dict[str, dict]):
    """Guarda el estado actual: {kb_path -> {"hash": str, "store_doc_id": str}}"""
    try:
        STATE_FILE.write_text(json.dumps(state, indent=2))
        logger.info(f"ğŸ’¾ sync_state.json guardado: {len(state)} documentos")
    except Exception as e:
        logger.error(f"âŒ Error saving sync_state.json: {e}")
        raise


# =========
# Main Sync Logic
# =========

def main():
    global STORE_NAME

    logger.info("=" * 70)
    logger.info("ğŸš€ SMART SYNC: KB â†’ File Search Store (con sync_state.json)")
    logger.info("=" * 70)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. Asegurar que existe el Store
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not STORE_NAME:
        logger.info("\nğŸ“¦ PASO 1: Creando nuevo File Search Store...")
        try:
            store = client.file_search_stores.create(
                config={"display_name": STORE_DISPLAY_NAME}
            )
            STORE_NAME = store.name
            logger.info(f"âœ… Store creado: {STORE_NAME}")
            logger.info(f"\nğŸ‘‰ IMPORTANTE: Guarda esto en tu .env:")
            logger.info(f"   FILE_SEARCH_STORE_NAME={STORE_NAME}")
        except Exception as e:
            logger.error(f"âŒ Error creando store: {e}")
            raise
    else:
        logger.info(f"\nâœ… PASO 1: Store existente: {STORE_NAME}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2. Cargar estado anterior (sync_state.json)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\nğŸ“‹ PASO 2: Cargando estado anterior...")
    old_state = load_sync_state()
    logger.info(f"   Documentos en sync_state.json: {len(old_state)}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3. Descubrir archivos .md en kb/ y calcular hashes
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\nğŸ“„ PASO 3: Explorando kb/ y calculando hashes...")
    md_files = sorted(KB_DIR.rglob("*.md"))
    md_files = [p for p in md_files if p.name.lower() != "template.md"]
    logger.info(f"   Archivos encontrados: {len(md_files)}")

    # Calcular hashes de archivos actuales
    current_hashes = {}
    for p in md_files:
        rel = p.relative_to(KB_DIR).as_posix()
        kb_path = f"kb/{rel}"
        content = p.read_text(encoding="utf-8", errors="ignore")
        current_hashes[kb_path] = sha256_text(content)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4. Procesamiento: NUEVO / CAMBIO / SIN CAMBIOS
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\nğŸ”„ PASO 4: Procesando cambios...")
    new_state = {}
    stats = {"uploaded": 0, "updated": 0, "unchanged": 0, "deleted": 0}

    for p in md_files:
        rel = p.relative_to(KB_DIR).as_posix()
        kb_path = f"kb/{rel}"
        new_hash = current_hashes[kb_path]

        logger.info(f"\n   ğŸ“„ {kb_path}")

        # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        # â•‘ CASO 1: Archivo existÃ­a antes                         â•‘
        # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if kb_path in old_state:
            old_entry = old_state[kb_path]
            old_hash = old_entry.get("hash")
            store_doc_id = old_entry.get("store_doc_id")

            # Subcase 1a: Sin cambios
            if new_hash == old_hash:
                logger.info(f"      âœ“ Sin cambios (hash igual)")
                new_state[kb_path] = old_entry  # Mantener Store ID
                stats["unchanged"] += 1
                continue

            # Subcase 1b: CambiÃ³ el contenido
            else:
                logger.info(f"      ğŸ”„ ACTUALIZACIÃ“N DETECTADA")
                logger.info(f"         Old hash: {old_hash[:16]}...")
                logger.info(f"         New hash: {new_hash[:16]}...")
                
                # Borrar documento viejo del Store (si tenemos su ID)
                if store_doc_id:
                    logger.info(f"      ğŸ—‘ï¸  Borrando documento obsoleto...")
                    delete_document(store_doc_id)
                    stats["deleted"] += 1  # â† Contar como eliminado (viejo)
                else:
                    # No tenemos ID (formato antiguo). Tratarlo como nuevo
                    logger.info(f"         (sin ID antiguo, tratando como nuevo)")
                
                stats["uploaded"] += 1  # â† Contar subida del nuevo

        # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        # â•‘ CASO 2: Archivo es NUEVO                              â•‘
        # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        else:
            logger.info(f"      â¬†ï¸  ARCHIVO NUEVO")
            stats["uploaded"] += 1

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Subir documento (NUEVO o reemplazo)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info(f"      â³ Subiendo a Store...")
        
        content = p.read_text(encoding="utf-8", errors="ignore")
        fm, _ = parse_frontmatter(content)
        
        # Construir metadata
        section = rel.split("/", 1)[0]
        meta = [
            {"key": "path", "string_value": kb_path},
            {"key": "section", "string_value": section},
            {"key": "hash", "string_value": new_hash},
        ]

        # Agregar campos del frontmatter
        for key in ["title", "description", "department", "doc_type", 
                    "owner_team", "maintainer", "visibility", "last_updated"]:
            value = fm.get(key)
            if value:
                meta.append({"key": key, "string_value": str(value)})

        # Agregar keywords si existen
        keywords = fm.get("keywords")
        if isinstance(keywords, list) and keywords:
            meta.append({"key": "keywords_csv", "string_value": ",".join([str(k) for k in keywords])})

        # Subir al Store
        try:
            response = client.file_search_stores.upload_to_file_search_store(
                file=str(p),
                file_search_store_name=STORE_NAME,
                config={
                    "display_name": kb_path,
                    "mime_type": "text/markdown",
                    "custom_metadata": meta,
                },
            )
            
            # response es una Operation, esperar a que complete
            import time
            operation = response
            max_wait = 60  # segundos
            waited = 0
            while not operation.done and waited < max_wait:
                time.sleep(2)
                operation = client.operations.get(operation.name)
                waited += 2
            
            if not operation.done:
                logger.warning(f"      âš ï¸ OperaciÃ³n no completÃ³ en {max_wait}s (continuando)")
            
            # DespuÃ©s de que se complete, buscar el documento que se creÃ³
            # Puede haber delay de propagaciÃ³n, reintentar
            store_doc_id = None
            for attempt in range(5):  # Reintentar hasta 5 veces
                docs = client.file_search_stores.documents.list(parent=STORE_NAME)
                for doc in docs:
                    for meta_item in doc.custom_metadata:
                        if meta_item.key == "path" and meta_item.string_value == kb_path:
                            # Encontramos un documento con este path
                            # Si hay multiple (viejo y nuevo), tomar el mÃ¡s reciente (Ãºltimamente creado)
                            if store_doc_id is None or doc.create_time > docs_by_path[-1].create_time:
                                store_doc_id = doc.name
                            break
                
                if store_doc_id and "documents/" in store_doc_id:
                    # Encontramos el document_id real
                    break
                elif attempt < 4:
                    logger.info(f"      â³ Esperando replicaciÃ³n del documento ({attempt+1}/5)...")
                    time.sleep(3)
            
            if not store_doc_id:
                # Fallback: usar el operation name si no encontramos el doc
                store_doc_id = operation.name
                logger.warning(f"      âš ï¸ No se encontrÃ³ document_id despuÃ©s de reintentos, usando operation_id")
            elif "upload/operations" in store_doc_id:
                logger.warning(f"      âš ï¸ Solo encontrÃ© operation_id, no document_id final")
            
            logger.info(f"      âœ… Subido exitosamente")
            logger.info(f"         Store ID: {store_doc_id[:60]}...")
            
            # Guardar en nuevo estado
            new_state[kb_path] = {
                "hash": new_hash,
                "store_doc_id": store_doc_id,
            }

        except Exception as e:
            logger.error(f"      âŒ Error subiendo: {e}")
            # Mantener entrada antigua si la habÃ­a
            if kb_path in old_state:
                new_state[kb_path] = old_state[kb_path]
            raise

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5. Detectar archivos ELIMINADOS (estaban antes, ya no existen)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\nğŸ—‘ï¸  PASO 5: Detectando eliminados...")
    for kb_path in old_state:
        if kb_path not in current_hashes:
            logger.info(f"   {kb_path}")
            logger.info(f"      âš ï¸ Path ya no existe en kb/")
            
            store_doc_id = old_state[kb_path].get("store_doc_id")
            if store_doc_id:
                delete_document(store_doc_id)
            stats["deleted"] += 1

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 6. Guardar nuevo estado
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\nğŸ’¾ PASO 6: Guardando nuevo estado...")
    save_sync_state(new_state)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 7. Resumen final
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    logger.info(f"\n" + "=" * 70)
    logger.info(f"ğŸ“Š RESUMEN DE SINCRONIZACIÃ“N:")
    logger.info(f"   â¬†ï¸  Nuevos:       {stats['uploaded']}")
    logger.info(f"   ğŸ”„ Actualizados: {stats['updated']}")
    logger.info(f"   âœ“ Sin cambios:   {stats['unchanged']}")
    logger.info(f"   ğŸ—‘ï¸  Eliminados:   {stats['deleted']}")
    logger.info(f"   ğŸ“š Total en Store: {len(new_state)}")
    logger.info(f"=" * 70)
    logger.info(f"\nâœ… Â¡SYNC COMPLETADO EXITOSAMENTE!")
    logger.info(f"\nğŸ‘‰ File Search Store ID:")
    logger.info(f"   {STORE_NAME}")
    logger.info(f"\nğŸ‘‰ Ãšsalo en la configuraciÃ³n del bot:")
    logger.info(f"   FILE_SEARCH_STORE_NAMES={STORE_NAME}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 8. Guardar cambios en Git (si estamos en CI/CD)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if os.getenv("CI") or os.getenv("GITHUB_ACTIONS"):
        logger.info(f"\nğŸ’¾ PASO 8: Guardando sync_state.json en Git...")
        try:
            import subprocess
            
            # Configurar git user (necesario en GitHub Actions)
            subprocess.run(["git", "config", "--global", "user.email", "sync@github.local"], check=False)
            subprocess.run(["git", "config", "--global", "user.name", "KB Sync Bot"], check=False)
            
            # Add the sync state file
            result_add = subprocess.run(["git", "add", str(STATE_FILE)], capture_output=True, text=True)
            if result_add.returncode != 0:
                logger.warning(f"   âš ï¸ Error en 'git add': {result_add.stderr}")
            
            # Verificar si hay cambios para commitear
            result_diff = subprocess.run(["git", "diff", "--cached", "--quiet"], capture_output=True)
            if result_diff.returncode != 0:  # Hay cambios (exit code 1 si hay diferencias)
                # Hacer commit
                result_commit = subprocess.run(
                    ["git", "commit", "-m", "chore: update sync_state.json after KB sync"],
                    capture_output=True,
                    text=True
                )
                if result_commit.returncode != 0:
                    logger.warning(f"   âš ï¸ Error en 'git commit': {result_commit.stderr}")
                else:
                    logger.info(f"   âœ“ Commit realizado")
                    
                    # Hacer push
                    result_push = subprocess.run(
                        ["git", "push", "origin", "main"],
                        capture_output=True,
                        text=True
                    )
                    if result_push.returncode != 0:
                        logger.warning(f"   âš ï¸ Error en 'git push': {result_push.stderr}")
                    else:
                        logger.info(f"   âœ… sync_state.json pusheado exitosamente")
            else:
                logger.info(f"   âœ“ No hay cambios en sync_state.json para commitear")
        except Exception as e:
            logger.warning(f"   âš ï¸ Error al procesar git operations: {e}")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error(f"\nâŒ FALLO FATAL: {e}")
        exit(1)

